# ğŸš€ Mini RAG System

A high-performance Retrieval-Augmented Generation (RAG) system built with React frontend and FastAPI backend, featuring citations, scores, and async processing.

## âœ¨ Features

- **ğŸ“š Document Ingestion**: Text upload and file processing with intelligent chunking
- **ğŸ” Vector Search**: Pinecone vector database with Google Gemini embeddings
- **ğŸ¯ Smart Reranking**: Cohere reranker for improved relevance
- **ğŸ¤– AI Answers**: Google Gemini Pro LLM with inline citations
- **ğŸ“Š Performance**: 200%+ speed improvement with async processing
- **ğŸ¨ Modern UI**: Responsive React frontend with drag & drop
- **ğŸ“ˆ Metrics**: Real-time performance tracking and token estimates

## ğŸ—ï¸ Architecture

```
Frontend (React) â†â†’ Backend (FastAPI) â†â†’ Vector DB (Pinecone)
                                    â†“
                              LLM (Google Gemini)
                              Reranker (Cohere)
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- Node.js 16+
- Pinecone account & API key
- Google AI API key
- Cohere API key

### 1. Backend Setup

```bash
cd rag/backend

# Install dependencies
pip install -r requirements.txt

# Set environment variables
cp .env.example .env
# Edit .env with your API keys

# Start the server
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### 2. Frontend Setup

```bash
cd rag

# Install dependencies
npm install

# Start development server
npm run dev
```

### 3. Test the System

```bash
cd rag/backend

# Test complete RAG pipeline
python test_complete_system.py

# Test individual components
python test_rag_pipeline.py
```

## ğŸ”§ Configuration

### Environment Variables

Create a `.env` file in the backend directory:

```bash
# Pinecone
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX=your_index_name

# Google AI
GOOGLE_API_KEY=your_google_api_key

# Cohere
COHERE_API_KEY=your_cohere_api_key

# Optional: Performance tuning
EMBEDDING_BATCH_SIZE=64
UPSERT_BATCH_SIZE=100
THREAD_POOL_WORKERS=4
```

### System Configuration

- **Chunk Size**: 1000 tokens (optimal for context retention)
- **Chunk Overlap**: 150 tokens (15% overlap for better retrieval)
- **Embedding Dimensions**: 768 (Google Gemini Embedding-001)
- **Vector Database**: Pinecone with cosine similarity
- **LLM Model**: Google Gemini Pro
- **Reranker**: Cohere rerank-english-v3.0

## ğŸ“± Frontend Components

### TextUpload
- File drag & drop support
- Text input with character limits
- Source and title metadata
- Upload progress tracking

### QueryBox
- Natural language query input
- Top-k result selection
- Relevance score inclusion
- Example query suggestions

### AnswerPanel
- AI-generated answers with citations
- Clickable citation markers [1], [2], [3]
- Source document snippets
- Performance metrics display

## ğŸ” API Endpoints

### Ingestion
- `POST /ingest` - Upload text with metadata
- `POST /ingest/sync` - Synchronous ingestion

### Query
- `POST /query` - Complete RAG pipeline
- `POST /query/sync` - Synchronous query
- `POST /batch-query` - Multiple queries

### System
- `GET /health` - Health check
- `GET /performance` - Performance metrics
- `GET /stats` - System statistics

## ğŸ§ª Testing

### Backend Tests

```bash
# Complete system test
python test_complete_system.py

# RAG pipeline test
python test_rag_pipeline.py

# Event loop fix test
python test_fix.py
```

### Frontend Testing

The React app includes comprehensive error handling and loading states. Test with:

1. **Text Upload**: Paste text or upload files
2. **Query Processing**: Ask questions about uploaded content
3. **Citation System**: Click on citation markers [1], [2], [3]
4. **Responsive Design**: Test on different screen sizes

## ğŸ“Š Performance Features

- **Async Processing**: 200%+ speed improvement
- **Batch Operations**: Optimized embedding and upsert batches
- **Connection Pooling**: Efficient API connections
- **Smart Caching**: In-memory caching for repeated operations
- **Parallel Processing**: Concurrent document processing

## ğŸš€ Deployment

### Backend (Render)

1. Connect your GitHub repository
2. Set environment variables
3. Deploy with Python runtime

### Frontend (Vercel/Netlify)

1. Connect repository
2. Set build command: `npm run build`
3. Deploy with React preset

## ğŸ” Troubleshooting

### Common Issues

1. **Import Errors**: Ensure all CSS files exist
2. **API Connection**: Check backend URL in frontend
3. **Environment Variables**: Verify all API keys are set
4. **CORS Issues**: Backend includes CORS middleware

### Performance Issues

1. **Slow Ingestion**: Reduce batch sizes
2. **Memory Issues**: Adjust thread pool workers
3. **API Limits**: Check rate limits for external APIs

## ğŸ“ˆ Monitoring

- **Health Checks**: `/health` endpoint
- **Performance Metrics**: `/performance` endpoint
- **System Stats**: `/stats` endpoint
- **Real-time Logs**: Backend console output

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- **Pinecone**: Vector database infrastructure
- **Google AI**: Embeddings and LLM services
- **Cohere**: Document reranking
- **FastAPI**: High-performance backend framework
- **React**: Modern frontend framework

## ğŸ“ Support

For issues and questions:
1. Check the troubleshooting section
2. Review the test scripts
3. Open a GitHub issue

---

**Built with â¤ï¸ for the AI community**
